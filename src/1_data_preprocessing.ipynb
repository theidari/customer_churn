{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5505496-e579-437f-9290-07463938a94e",
   "metadata": {},
   "source": [
    "<h1>1- Data Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344dcd4f-272f-4b8d-92d9-3728e5df799b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The helpers file is imported ☑\n",
      "The plots file is imported ☑\n",
      "The classes file is imported ☑\n"
     ]
    }
   ],
   "source": [
    "# Dependencies and Setup\n",
    "from google.cloud import bigquery\n",
    "from package.helpers import *\n",
    "from package.plots import *\n",
    "from package.classes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e7555b-df04-45d7-806f-325fbb07d136",
   "metadata": {},
   "source": [
    "<h3>1-1- Data Collection</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a12832-f435-4934-8dba-870de2d259a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to handle data loading from BigQuery and retrieval in Spark\n",
    "class DataToSpark():\n",
    "    def __init__(self, project_id, dataset_id):\n",
    "        # Initialize the class with the specified project and dataset identifiers\n",
    "        self.project_id = project_id\n",
    "        self.dataset_id = dataset_id\n",
    "\n",
    "    def load_tables(self, tables = []):\n",
    "        # Load tables into DataFrames and store them in a dictionary\n",
    "        self.df_tables = {}  # Dictionary to store DataFrames\n",
    "        for i, table in enumerate(tables):\n",
    "            df_table = spark.read.format(\"bigquery\").option(\"table\", f\"{self.project_id}:{self.dataset_id}.{table}\").load()\n",
    "            self.df_tables[table] = df_table\n",
    "\n",
    "    def get_tables(self):\n",
    "        # Retrieve the stored DataFrames\n",
    "        return self.df_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80ce3dde-7af0-401e-847d-66ee578ceedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "member\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---+------+--------------+----------------------+\n",
      "|                msno|city| bd|gender|registered_via|registration_init_time|\n",
      "+--------------------+----+---+------+--------------+----------------------+\n",
      "|icRXKGc8YLAIcS6nu...|   1|  0|  null|             7|              20120320|\n",
      "+--------------------+----+---+------+--------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "user\n",
      "+--------------------+--------+------+------+------+-------+-------+-------+----------+\n",
      "|                msno|    date|num_25|num_50|num_75|num_985|num_100|num_unq|total_secs|\n",
      "+--------------------+--------+------+------+------+-------+-------+-------+----------+\n",
      "|FnqNUBvN8mysLeKba...|20160119|    34|     1|     1|      1|     88|    108| 24172.471|\n",
      "+--------------------+--------+------+------+------+-------+-------+-------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "transactions\n",
      "+--------------------+-----------------+-----------------+---------------+------------------+-------------+----------------+----------------------+---------+\n",
      "|                msno|payment_method_id|payment_plan_days|plan_list_price|actual_amount_paid|is_auto_renew|transaction_date|membership_expire_date|is_cancel|\n",
      "+--------------------+-----------------+-----------------+---------------+------------------+-------------+----------------+----------------------+---------+\n",
      "|N13/tGXlfPZCYcDEZ...|                2|              195|            894|               894|            0|        20150126|              20150809|        0|\n",
      "+--------------------+-----------------+-----------------+---------------+------------------+-------------+----------------+----------------------+---------+\n",
      "only showing top 1 row\n",
      "\n",
      "train\n",
      "+--------------------+--------+\n",
      "|                msno|is_churn|\n",
      "+--------------------+--------+\n",
      "|waLDQMmcOu2jLDaV1...|       1|\n",
      "+--------------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the 'DataToSpark' class to handle KKBOX data\n",
    "kkbox = DataToSpark(\"customer-churn-391917\", \"kkbox\")\n",
    "\n",
    "# Load specific tables into DataFrames\n",
    "tables_to_load = [\"member\", \"user\", \"transactions\", \"train\"]\n",
    "kkbox.load_tables(tables_to_load)\n",
    "\n",
    "# Retrieve the stored DataFrames\n",
    "tables = kkbox.get_tables()\n",
    "\n",
    "# Display the first row of each table DataFrame\n",
    "for table_name, table_df in tables.items():\n",
    "    print(table_name)\n",
    "    table_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc33cdc-0a69-4016-96a4-5908bba6b8cc",
   "metadata": {},
   "source": [
    "<h3>2-1- Exploratory Data Analysis (EDA)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac50c19-608f-4455-88bf-5479ddbe8d9e",
   "metadata": {},
   "source": [
    "<h4>1-2-1- Members Dataset</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78c9b22f-1369-4f16-8510-9fa203c1a9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape\n",
      " |-- rows: 992931\n",
      " |-- columns: 7\n",
      "root\n",
      " |-- msno: string (nullable = true)\n",
      " |-- is_churn: long (nullable = true)\n",
      " |-- city: long (nullable = true)\n",
      " |-- bd: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- registered_via: long (nullable = true)\n",
      " |-- registration_init_time: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform the merge operation for train and members dataset\n",
    "merge_df = tables[\"train\"].join(tables[\"member\"], on=['msno'], how='left')\n",
    "\n",
    "# Dataframe shape\n",
    "print(f\"shape\\n |-- rows: {merge_df.count()}\\n |-- columns: {len(merge_df.columns)}\")\n",
    "merge_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3844ac1b-ce1c-49a5-ab9d-485edae00842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the \"registration_init_time\" column\n",
    "merge_df = merge_df.withColumn(\"registration_init_time\",\n",
    "                               when(col(\"registration_init_time\").isNotNull(),\n",
    "                                    to_date(col(\"registration_init_time\").cast(\"string\"), \"yyyyMMdd\")\n",
    "                                   ).otherwise(\"NAN\"))\n",
    "\n",
    "# Fill missing values in the \"gender\" column with \"NAN\"\n",
    "merge_df = merge_df.fillna(\"NAN\", subset=[\"gender\"])\n",
    "\n",
    "# Columns to replace null values with \"NAN\" and cast to numeric\n",
    "columns_to_replace = [\"city\", \"bd\", \"registered_via\"]\n",
    "for column in columns_to_replace:\n",
    "    merge_df = merge_df.withColumn(column, when(col(column).isNull(), \"NAN\").otherwise(col(column).cast(\"double\")))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "# merge_df.show()\n",
    "\n",
    "# # Print the schema of the updated DataFrame\n",
    "# merge_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c46f1-acb1-43fa-beb8-d1c580c04717",
   "metadata": {},
   "source": [
    "<h4>1-1-2-1- Data and Model Noise</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818763fd-7617-4fb6-b25a-ee86aec6540d",
   "metadata": {},
   "source": [
    "<p align=\"justify\"><b>a)</b> The data is absent in all columns (city, bd, gender, registered_via, and registration_init_time), where they are marked as NAN.</p>\n",
    "<p align=\"justify\"><b>b)</b> Instances of duplicated data are identified by considering attributes such as city, bd, gender, and registered_via, with specific attention placed on scenarios where bd equals 0 and gender is labeled as NAN.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633818f-eabb-4d71-bf9e-5535934465b6",
   "metadata": {},
   "source": [
    "<b><ul><li>Noise a evaluation</li></ul></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e579125c-eca8-4ff1-8a9b-f946bd43c5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=====================================================>(197 + 3) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n",
      " |-- count: 115770\n",
      " |-- out of total data: 11.66 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter out rows with all columns set to \"NAN\"\n",
    "NAN_filtered_df = merge_df.filter((col(\"city\") == \"NAN\") &\n",
    "                                  (col(\"bd\") == \"NAN\") &\n",
    "                                  (col(\"gender\") == \"NAN\") &\n",
    "                                  (col(\"registered_via\") == \"NAN\") &\n",
    "                                  (col(\"registration_init_time\") == \"NAN\"))\n",
    "\n",
    "# Calculate and print the filtered result count and percentage\n",
    "print(f\"result\\n |-- count: {NAN_filtered_df.count()}\\n |-- out of total data: {round(NAN_filtered_df.count()/merge_df.count(),4)*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61615cca-0861-4395-9bc6-c26e4ed1f9f6",
   "metadata": {},
   "source": [
    "<b><ul><li>Noise b evaluation</li></ul></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00c32b75-370e-4270-8a71-552fef9d5531",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:===================================================>  (191 + 4) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+--------------+------+\n",
      "|city| bd|gender|registered_via| count|\n",
      "+----+---+------+--------------+------+\n",
      "| 1.0|0.0|   NAN|           7.0|399071|\n",
      "| NAN|NAN|   NAN|           NAN|115770|\n",
      "| 1.0|0.0|   NAN|           4.0| 16453|\n",
      "| 1.0|0.0|   NAN|           9.0| 15651|\n",
      "| 1.0|0.0|   NAN|           3.0|  6786|\n",
      "|13.0|0.0|   NAN|           9.0|  4072|\n",
      "| 1.0|0.0|   NAN|          13.0|  2804|\n",
      "| 5.0|0.0|   NAN|           9.0|  2660|\n",
      "|13.0|0.0|   NAN|           3.0|  2399|\n",
      "|15.0|0.0|   NAN|           9.0|  1635|\n",
      "+----+---+------+--------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Group the data and count occurrences based on specified columns\n",
    "unit_count = merge_df.groupBy(\"city\", \"bd\", \"gender\", \"registered_via\").count()\n",
    "\n",
    "# Order the grouped data by count in descending order and show the top 10\n",
    "top_10_unit_count = unit_count.orderBy(col(\"count\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a63d2da4-1202-46c7-8064-bca1397e2d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:=====================================================>(197 + 3) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n",
      " |-- count: 455389\n",
      " |-- out of total data: 45.86 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# city=1 assesment\n",
    "city_1 = merge_df.filter(col(\"city\") == 1)\n",
    "\n",
    "# Calculate and print the filtered result count and percentage\n",
    "print(f\"result\\n |-- count: {city_1.count()}\\n |-- out of total data: {round(city_1.count()/merge_df.count(),4)*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc5aa8-2f98-4a74-a20c-09a63ebedf0c",
   "metadata": {},
   "source": [
    "<h4>2-1-2-1- Observation</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f3834-6d4e-4ab8-9b37-2596847dd337",
   "metadata": {},
   "source": [
    "<p align=\"justify\">The member dataset exhibits a presence of 11.66% null values that appear to be attributable to inherent noise. To ensure data quality, it is recommended to undertake the removal of rows with complete null values. Further scrutiny reveals that 45.86% of the dataset originates from a specific city (city 1), yet lacks essential demographic details such as age and gender. This scenario gives rise to an imbalanced dataset, characterized by a substantial disparity in the representation of different cities. This imbalance can potentially introduce bias in machine learning model performance, leading it to disproportionately emphasize the predominant class while marginalizing the less prevalent classes. To mitigate this challenge, a judicious approach involves the development of a separate model dedicated to the analysis of city 1's data, thus fostering a more nuanced and equitable learning process.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6666daa8-774f-499d-8d52-7bdc04839714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame 'member_model_df' by subtracting the contents of 'NAN_filtered_df' and 'city_1' from 'merge_df'.\n",
    "member_model_df = merge_df.subtract(NAN_filtered_df).subtract(city_1)\n",
    "\n",
    "# Dataframe shape\n",
    "# print(f\"shape\\n |-- rows: {member_model_df.count()}\\n |-- columns: {len(member_model_df.columns)}\")\n",
    "# member_model_df.printSchema()\n",
    "\n",
    "# # Display the resulting 'member_model_df'.\n",
    "# member_model_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9948b320-2fd9-4cc4-9bf8-5143a873b063",
   "metadata": {},
   "source": [
    "<h4>2-2-1- Transaction Data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f755e4-6331-4799-baaa-1177ad3367cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the merge operation for members output dataset and Transaction data\n",
    "transactions_merge_df = member_model_df.join(tables[\"transactions\"], on=['msno'], how='left')\n",
    "\n",
    "# Dataframe shape\n",
    "print(f\"shape\\n |-- rows: {transactions_merge_df.count()}\\n |-- columns: {len(transactions_merge_df.columns)}\")\n",
    "transactions_merge_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15931c22-39d7-4ba1-b00c-23cf1999fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window specification for ranking transactions within each member group\n",
    "window_spec = Window.partitionBy(\"msno\").orderBy(F.col(\"transaction_date\").desc())\n",
    "\n",
    "# Rank transactions within each member group\n",
    "ranked_df = transactions_merge_df.withColumn(\"rank\", F.rank().over(window_spec))\n",
    "\n",
    "# Select rows with rank 1 (last transaction for each member)\n",
    "last_transaction_df = ranked_df.filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# Select rows with rank 2 (transaction before the last for each member)\n",
    "transaction_before_last_df = ranked_df.filter(F.col(\"rank\") == 2).drop(\"rank\")\n",
    "\n",
    "# Select specific columns and rename one of them\n",
    "selected_columns_df = transaction_before_last_df.select(\n",
    "    F.col(\"msno\"),\n",
    "    F.col(\"membership_expire_date\").alias(\"exp_last\")\n",
    ")\n",
    "\n",
    "# Merge members' output dataset with transaction data\n",
    "new_tran_df = last_transaction_df.join(selected_columns_df, on=['msno'], how='left')\n",
    "\n",
    "# Dataframe shape\n",
    "print(f\"shape\\n |-- rows: {new_tran_df.count()}\\n |-- columns: {len(new_tran_df.columns)}\")\n",
    "new_tran_df.printSchema()\n",
    "\n",
    "# Show the result\n",
    "# new_tran_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f353d79-91e5-4447-88e4-109cda899b62",
   "metadata": {},
   "source": [
    "<h4>1-2-2-1- Data and Model Noise</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef2fcca-b30e-4b3d-9340-46d9eadf61b0",
   "metadata": {},
   "source": [
    "<p align=\"justify\"><b>a)</b> The 'msno' column has matching 'transaction_date' and 'membership_expire_date' values, but differs in other columns such as errors in the 'is_cancel' column, which are distinct for the below example. In this context, the 'exp_last' value is null.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaf16e5-4f9f-4db4-9fb6-964869a90e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering rows where \"msno\" matches a specific value\n",
    "noise_a = new_tran_df.filter(F.col(\"msno\") == \"1M+OGzETqoIR33bo2mzrTP3p8jOFyVdUX5vJ3KLe2ms=\")\n",
    "\n",
    "# Displaying the filtered results\n",
    "# noise_a.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e12b11d-053c-4095-b3bc-b7055610f336",
   "metadata": {},
   "source": [
    "<p align=\"justify\"><b>b)</b> The \"msno\" shares identical \"transaction_date\" and \"membership_expire_date,\" but has errors in the \"exp_last\" column. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340a4b9-85a9-420d-800e-425babda60f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering rows where \"msno\" matches a specific value\n",
    "noise_b = new_tran_df.filter(col(\"msno\") == \"ya/pbMnE1Bc9NXQIQ3r9avpXJet0hiNQEgy8QMG98ZI=\")\n",
    "\n",
    "# Displaying the filtered results\n",
    "# noise_b.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe1869-3034-4ec7-88bc-a4f99bf0a301",
   "metadata": {},
   "source": [
    "<p align=\"justify\"><b>c)</b> The \"msno\" has one \"transaction_date\" and multiple distinct \"membership_expire_date\" values..</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a4420-64d3-4469-b7fc-1b3c83ff83fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering rows where \"msno\" matches a specific value\n",
    "noise_c = new_tran_df.filter(col(\"msno\") == \"pI0cMv4wwhvLTBJpJSoHQrG6pdazfXo77JmRfKoyA6U=\")\n",
    "\n",
    "# Displaying the filtered results\n",
    "# noise_c.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055cc422-803d-45c8-afc3-24683d969f0c",
   "metadata": {},
   "source": [
    "<b><ul><li>Noise evaluation</li></ul></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f8a388-3a9d-42e4-af10-8540ede0a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_finder(df, param=[]):\n",
    "    \"\"\"\n",
    "    Finds and filters noise in a DataFrame based on specified columns.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        param (list): List of column names for grouping and filtering.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered DataFrame with noise removed.\n",
    "        DataFrame: Information about duplicate rows.\n",
    "        DataFrame: \"msno\" keys for noise identification.\n",
    "    \"\"\"\n",
    "    # Grouping and counting duplicate rows based on specific columns\n",
    "    noise_duplicate_rows = df.groupBy(param).count().orderBy(F.desc(\"count\"))\n",
    "\n",
    "    # Filtering rows with a count greater than 1 (i.e., duplicate rows)\n",
    "    noise_duplicate_rows = noise_duplicate_rows.filter(F.col(\"count\") > 1)\n",
    "\n",
    "    # Selecting the \"msno\" column from the DataFrame with duplicate rows\n",
    "    noise_msno_keys = noise_duplicate_rows.select(\"msno\")\n",
    "    \n",
    "    # Joining the original DataFrame with the selected \"msno\" keys\n",
    "    noise_filtered_df = df.join(noise_msno_keys, on=[\"msno\"], how=\"inner\")\n",
    "\n",
    "#     # Print DataFrame shape information\n",
    "#     print(f\"DataFrame Shape:\\n |-- Number of Records: {noise_filtered_df.count()}\\n |-- Number of Unique Cases: {noise_duplicate_rows.count()}\")\n",
    "\n",
    "#     # Displaying the resulting DataFrame after filtering\n",
    "#     noise_filtered_df.show()\n",
    "    \n",
    "    return noise_filtered_df, noise_duplicate_rows, noise_msno_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38184724-e66a-4d00-9fe0-da967a189f3e",
   "metadata": {},
   "source": [
    "<b><ul><li>Noise a and b evaluation</li></ul></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164dc3c4-4120-4886-b399-7ce71da90f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_a_filtered_df, noise_a_duplicate_rows, noise_a_msno_keys = noise_finder(new_tran_df, [\"msno\", \"transaction_date\", \"membership_expire_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f40594-680d-4992-b5ce-e9873f90eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping and counting duplicate rows based on specific columns\n",
    "noise_a_duplicate_rows = new_tran_df.groupBy(\"msno\", \"transaction_date\", \"membership_expire_date\").count().orderBy(F.desc(\"count\"))\n",
    "\n",
    "# Filtering rows with a count greater than 1 (i.e., duplicate rows)\n",
    "noise_a_duplicate_rows = noise_a_duplicate_rows.filter(F.col(\"count\") > 1)\n",
    "\n",
    "# Dataframe shape\n",
    "print(f\"shape\\n |-- rows: {noise_a_duplicate_rows.count()}\\n |-- columns: {len(noise_a_duplicate_rows.columns)}\")\n",
    "\n",
    "# Displaying the results with full column content\n",
    "# noise_a_duplicate_rows.show(truncate=False)\n",
    "\n",
    "# Selecting the \"msno\" column from the DataFrame with duplicate rows\n",
    "noise_a_msno_keys = noise_a_duplicate_rows.select(\"msno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72367bd-d647-448c-a553-365468d254d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the original DataFrame with the selected \"msno\" keys\n",
    "noise_a_filtered_df = new_tran_df.join(noise_a_msno_keys, on=[\"msno\"], how=\"inner\")\n",
    "\n",
    "# Dataframe shape\n",
    "print(f\"shape\\n |-- rows: {noise_a_filtered_df.count()}\\n |-- columns: {len(noise_a_filtered_df.columns)}\")\n",
    "\n",
    "# Displaying the resulting DataFrame after filtering\n",
    "# noise_a_filtered_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65436284-190d-4f26-9c23-6448d63c4d35",
   "metadata": {},
   "source": [
    "<b><ul><li>Noise c evaluation</li></ul></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824f9d6-c357-420c-a3aa-1dcdf99a315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_c_filtered_df, noise_c_duplicate_rows, noise_c_msno_keys = noise_finder(new_tran_df, [\"msno\", \"transaction_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e356a6e6-f82b-4ef8-bb0f-a1dce96b854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping and counting duplicate rows based on \"msno\" and \"transaction_date\" columns\n",
    "noise_c_duplicate_rows = new_tran_df.groupBy(\"msno\", \"transaction_date\").count().orderBy(F.desc(\"count\"))\n",
    "\n",
    "# Filtering rows with a count greater than 1 (i.e., duplicate rows)\n",
    "noise_c_duplicate_rows = noise_c_duplicate_rows.filter(F.col(\"count\") > 1)\n",
    "\n",
    "# Dataframe shape\n",
    "print(f\"shape\\n |-- rows: {noise_a_duplicate_rows.count()}\\n |-- columns: {len(noise_a_duplicate_rows.columns)}\")\n",
    "\n",
    "# Displaying the results with full column content\n",
    "# noise_c_duplicate_rows.show(truncate=False)\n",
    "\n",
    "# Selecting the \"msno\" column from the DataFrame with duplicate rows\n",
    "noise_c_msno_keys = noise_c_duplicate_rows.select(\"msno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e7a51-3933-4502-af02-875a3431cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the original DataFrame with the selected \"msno\" keys\n",
    "noise_c_filtered_df = new_tran_df.join(noise_c_msno_keys, on=[\"msno\"], how=\"inner\")\n",
    "\n",
    "# Dataframe shape\n",
    "print(f\"shape\\n |-- rows: {noise_c_filtered_df.count()}\\n |-- columns: {len(noise_c_filtered_df.columns)}\")\n",
    "\n",
    "# Displaying the resulting DataFrame after filtering\n",
    "# noise_c_filtered_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f92a6a6-2861-44b8-9f1d-0fd5aa8bf272",
   "metadata": {},
   "source": [
    "<h4>2-2-2-1- Observation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b6d5e-2997-498f-9534-02bd445553cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame 'trans_model_df' by subtracting the contents of 'noise_a_filtered_df' and 'noise_c_filtered_df' from 'merge_df'.\n",
    "trans_model_df = new_tran_df.subtract(noise_a_filtered_df).subtract(noise_c_filtered_df)\n",
    "\n",
    "# Dataframe shape\n",
    "print(f\"shape\\n |-- rows: {trans_model_df.count()}\\n |-- columns: {len(trans_model_df.columns)}\")\n",
    "trans_model_df.printSchema()\n",
    "\n",
    "# Display the resulting 'trans_model_df'.\n",
    "# trans_model_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc301dd5-e014-47b7-a801-5e52d5df52e4",
   "metadata": {},
   "source": [
    "<h4>3-2-1- User Data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541ffc5c-330c-4348-af7f-a91cc234c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter activity data to just the closest to last data (january 2017)\n",
    "user_filter_df=tables[\"user\"].filter((col(\"date\") >= \"20170101\") & (col(\"date\") <= \"20170131\"))\n",
    "\n",
    "# Selecting the \"msno\" column from the \"trans_model_df\" DataFrame\n",
    "trans_msno_keys = trans_model_df.select(\"msno\")\n",
    "\n",
    "user_merge_df = user_filter_df.join(trans_msno_keys, on=[\"msno\"], how=\"inner\")\n",
    "\n",
    "# Dataframe shape\n",
    "print(f\"shape\\n |-- rows: {user_merge_df.count()}\\n |-- columns: {len(user_merge_df.columns)}\")\n",
    "user_merge_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff80c0-2319-4e15-bab5-13ea40e7120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sum_df = user_merge_df.groupBy(\"msno\").agg(\n",
    "    F.format_number(F.count('date'),0).alias('activity'),\n",
    "    F.sum(\"num_25\").alias(\"sum_num_25\"),\n",
    "    F.sum(\"num_50\").alias(\"sum_num_50\"),\n",
    "    F.sum(\"num_75\").alias(\"sum_num_75\"),\n",
    "    F.sum(\"num_985\").alias(\"sum_num_985\"),\n",
    "    F.sum(\"num_100\").alias(\"sum_num_100\"),\n",
    "    F.sum(\"num_unq\").alias(\"sum_num_unq\"),\n",
    "    F.sum(\"total_secs\").alias(\"total_secs\"))\n",
    "\n",
    "# Dataframe shape\n",
    "print(f\"shape\\n |-- rows: {user_sum_df.count()}\\n |-- columns: {len(user_sum_df.columns)}\")\n",
    "user_sum_df.printSchema()\n",
    "\n",
    "# user_sum_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20781678-b4d6-46e3-927c-97cb7bddd7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = trans_model_df.join(user_sum_df, on=[\"msno\"], how=\"inner\")\n",
    "\n",
    "# Dataframe shape\n",
    "print(f\"shape\\n |-- rows: {model_df.count()}\\n |-- columns: {len(model_df.columns)}\")\n",
    "model_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe8332-ea14-4800-a2da-d35c5838d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56539091-9fd5-4848-9044-f5b42960cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable for credentials\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = './customer-churn-391917-150cab3a2647.json'\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Save to GCS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Assuming you already have the DataFrame model_df\n",
    "# model_df = ...\n",
    "\n",
    "# Specify GCS path\n",
    "gcs_bucket = \"gs://kkbox_data_churn\"\n",
    "file_name = \"model_main\"\n",
    "gcs_path = f\"{gcs_bucket}/{file_name}\"\n",
    "\n",
    "# Save DataFrame to GCS\n",
    "model_df.write.mode('overwrite').parquet(gcs_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "local-pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
